#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Unified Site & Sector Locator Tool (NO-ML & ML with Continual Training) — v3
============================================================================
- NO-ML: physics/geometry baseline (weighted centroid + azimuth hist), optional TA refine, soft 360/N spacing.
- ML: distance regressor + geometric solver, **continual training** (replay dataset + versioned bundles),
      **NaN-safe** via SimpleImputer, and **site-merge** so that all sectors of a site share the SAME lat/lon.

New in v3 (this build):
- Fixed weighted centroid bug (subset weights).
- Stronger geofences in NO-ML and optional site merging controlled by --merge-sites.
- ML pipeline now **merges sectors to site center (median)** by default (can disable with --no-ml-merge).
- ML optionally computes **eval metrics** (MAE/RMSE in meters) against a labeled eval file.
- Saves a per-sector ML CSV for debugging, plus site-merged CSV.
"""
import argparse, os, sys, math, re, logging, hashlib, json
from datetime import datetime
from typing import Dict, Tuple, List
import numpy as np
import pandas as pd

# Optional ML imports
try:
    from sklearn.ensemble import RandomForestRegressor
    from sklearn.model_selection import KFold
    from sklearn.metrics import mean_absolute_error
    from sklearn.impute import SimpleImputer
    import joblib
    SKLEARN_AVAILABLE = True
except Exception:
    SKLEARN_AVAILABLE = False

# ----------------------------- Logging -----------------------------
def setup_logger(outdir: str, tag: str="run"):
    os.makedirs(outdir, exist_ok=True)
    log_path = os.path.join(outdir, f"{tag}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log")
    logging.basicConfig(
        filename=log_path, filemode="w", level=logging.INFO,
        format="%(asctime)s [%(levelname)s] %(message)s"
    )
    console = logging.StreamHandler()
    console.setLevel(logging.INFO)
    console.setFormatter(logging.Formatter("%(message)s"))
    logging.getLogger().addHandler(console)
    logging.info(f"Log file: {log_path}")
    return log_path

# ----------------------------- Utils -------------------------------
def normalize_cols(cols):
    out = []
    for c in cols:
        cc = str(c).strip()
        cc = cc.replace("/", "_per_").replace("-", "_").replace("(", "").replace(")", "").replace(" ", "_")
        cc = re.sub("_+", "_", cc)
        out.append(cc.lower())
    return out

def to_num(x):
    try:
        if x is None: return np.nan
        if isinstance(x, (int,float,np.number)): v = float(x)
        else: v = float(str(x).strip().replace(",",""))
        if abs(v - 2147483647.0) < 1: return np.nan
        if math.isinf(v): return np.nan
        return v
    except: return np.nan

def deg2rad(d): return d*math.pi/180.0
def rad2deg(r): return r*180.0/math.pi
def haversine(lat1, lon1, lat2, lon2):
    R=6371000.0
    phi1, phi2 = deg2rad(lat1), deg2rad(lat2)
    dphi = deg2rad(lat2-lat1)
    dl = deg2rad(lon2-lon1)
    a = math.sin(dphi/2)**2 + math.cos(phi1)*math.cos(phi2)*math.sin(dl/2)**2
    return 2*R*math.asin(math.sqrt(a))

def meters_to_offsets(dNorth, dEast, base_lat):
    R=6378137.0
    dLat = dNorth / R
    dLon = dEast / (R * math.cos(math.pi * base_lat / 180.0))
    latO = base_lat + dLat * 180.0 / math.pi
    lonO = dLon * 180.0 / math.pi
    return latO, lonO

def bearing_from_site(lat_site, lon_site, lat, lon):
    phi1, phi2 = deg2rad(lat_site), deg2rad(lat)
    dlon = deg2rad(lon - lon_site)
    y = math.sin(dlon) * math.cos(phi2)
    x = math.cos(phi1)*math.cos(phi2)*math.cos(dlon) + math.sin(phi1)*math.sin(phi2)
    b = math.atan2(y, x)
    return (rad2deg(b) + 360.0) % 360.0

def snap_deg(x, step=5):
    try:
        if np.isnan(x): return np.nan
        return (int(round(float(x)/step))*step) % 360
    except: return np.nan

def infer_site_key(cellid):
    if pd.isna(cellid): return np.nan
    s = str(cellid).strip()
    if re.fullmatch(r"\d+", s):
        try:
            v = int(s); return v // 10
        except:
            return s[:-1] if len(s)>1 else s
    else:
        return s[:-1] if len(s)>1 else s

def load_any(path: str, sheet: str = None) -> pd.DataFrame:
    if path.lower().endswith((".xlsx",".xls")):
        return pd.read_excel(path, sheet_name=sheet) if sheet else pd.read_excel(path)
    return pd.read_csv(path)

def standardize_df(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()
    df.columns = normalize_cols(df.columns)
    
    # Latitude/Longitude mapping
    lat_candidates = ["latitude", "lat", "sector_lat", "site_lat"]
    for candidate in lat_candidates:
        if candidate in df.columns and "lat" not in df.columns:
            df.rename(columns={candidate: "lat"}, inplace=True)
            logging.info(f"Mapped '{candidate}' -> 'lat'")
            break
    
    lon_candidates = ["longitude", "lon", "sector_lon", "site_lon"]
    for candidate in lon_candidates:
        if candidate in df.columns and "lon" not in df.columns:
            df.rename(columns={candidate: "lon"}, inplace=True)
            logging.info(f"Mapped '{candidate}' -> 'lon'")
            break
    
    # EARFCN/NARFCN mapping
    earfcn_candidates = ["earfcn", "narfcn", "arfcn", "uarfcn", "frequency_channel", "channel"]
    for candidate in earfcn_candidates:
        if candidate in df.columns and "earfcn_or_narfcn" not in df.columns:
            df.rename(columns={candidate: "earfcn_or_narfcn"}, inplace=True)
            logging.info(f"Mapped '{candidate}' -> 'earfcn_or_narfcn'")
            break
    
    # PCI/PSI mapping
    pci_candidates = ["pci", "psi", "physical_cell_id", "physcellid", "pcid", "primary_scrambling_code", "psc"]
    for candidate in pci_candidates:
        if candidate in df.columns and "pci_or_psi" not in df.columns:
            df.rename(columns={candidate: "pci_or_psi"}, inplace=True)
            logging.info(f"Mapped '{candidate}' -> 'pci_or_psi'")
            break
    
    # Network mapping (if exists)
    network_candidates = ["network", "operator", "carrier", "mno"]
    for candidate in network_candidates:
        if candidate in df.columns and "network" not in df.columns:
            df.rename(columns={candidate: "network"}, inplace=True)
            logging.info(f"Mapped '{candidate}' -> 'network'")
            break
    
    # Numeric conversions
    for c in ["lat", "lon", "rsrp_dbm", "rsrq_db", "sinr_db", "rssi", "earfcn_or_narfcn", "pci_or_psi", "band_mhz", "speed_kmh", "heading_deg", "ta"]:
        if c in df.columns:
            df[c] = df[c].apply(to_num)
    
    # Categorical conversions
    for cat in ["network", "technology"]:
        if cat in df.columns:
            df[cat] = df[cat].astype(str).str.lower()
    
    # Log final column mapping
    required = ["lat", "lon", "earfcn_or_narfcn", "pci_or_psi"]
    missing = [col for col in required if col not in df.columns]
    if missing:
        logging.warning(f"Missing required columns after mapping: {missing}")
        logging.info(f"Available columns: {list(df.columns)}")
    else:
        logging.info(f"✅ All required columns mapped successfully")
    
    return df
# --------------------- Shared Azimuth ------------------------------
def azimuth_histogram(samples: pd.DataFrame, lat_site: float, lon_site: float, bin_size:int=5):
    if "rsrp_dbm" in samples.columns and not samples["rsrp_dbm"].isna().all():
        w = samples["rsrp_dbm"].apply(lambda v: max(10**(v/10.0), 1e-13) if pd.notna(v) else 0.0).values
    else:
        w = np.ones(len(samples))
    bearings = np.array([bearing_from_site(lat_site, lon_site, r.lat, r.lon) for r in samples.itertuples(index=False)])
    dists = np.array([haversine(lat_site, lon_site, r.lat, r.lon) for r in samples.itertuples(index=False)])
    w2 = w * np.power(np.maximum(dists, 1.0), 0.5)
    bins = np.arange(0, 360+bin_size, bin_size)
    hist, edges = np.histogram(bearings, bins=bins, weights=w2)
    if hist.sum() <= 0:
        return np.nan, np.nan, 0.0
    peak_idx = int(np.argmax(hist))
    center_deg = (edges[peak_idx] + edges[peak_idx+1]) / 2.0
    half = hist[peak_idx] / 2.0
    n = len(hist)
    left = peak_idx
    while hist[left] >= half:
        left = (left - 1) % n
        if left == (peak_idx - 1) % n: break
    right = peak_idx
    while hist[right] >= half:
        right = (right + 1) % n
        if right == (peak_idx + 1) % n: break
    width_bins = (right - left) if right >= left else (n - left + right)
    beam_deg = width_bins * bin_size
    reliability = float(hist[peak_idx]/max(hist.sum(),1e-12))
    return snap_deg(center_deg, step=bin_size), beam_deg, reliability

def soft_equal_spacing(df_site: pd.DataFrame, bin_size:int=5) -> pd.DataFrame:
    def ang_diff(a,b):
        return (a - b + 180.0) % 360.0 - 180.0
    N = int(df_site["pci_or_psi"].nunique())
    if N <= 1:
        df_site["azimuth_deg_5_soft"] = df_site["azimuth_deg_5"]
        df_site["azimuth_adjustment_deg"] = 0.0
        df_site["template_spacing_deg"] = np.nan
        df_site["spacing_used"] = "none"
        return df_site
    S = 360.0 / N
    rows = df_site.sort_values("azimuth_deg_5").reset_index(drop=True).copy()
    meas = rows["azimuth_deg_5"].astype(float).values
    if "azimuth_reliability" in rows.columns and not rows["azimuth_reliability"].isna().all():
        w = rows["azimuth_reliability"].fillna(0.4).clip(0.05, 1.0).values
    else:
        if "samples" in rows.columns:
            w = (rows["samples"] / max(rows["samples"].max(), 1)).clip(0.05, 1.0).values
        else:
            w = np.ones(len(rows))
    best_cost, best = 1e18, None
    for offset in range(N):
        theta0_candidates = [ (meas[i] - ((i - offset) % N)*S) % 360.0 for i in range(N) ]
        angs = np.radians(theta0_candidates)
        C = np.sum(w * np.cos(angs)); S_sin = np.sum(w * np.sin(angs))
        theta0 = 0.0 if (C==0 and S_sin==0) else (np.degrees(np.arctan2(S_sin, C)) % 360.0)
        cost = 0.0
        for i in range(N):
            k = (i - offset) % N
            target = (theta0 + k*S) % 360.0
            d = abs(ang_diff(meas[i], target)); cost += w[i]*d
        if cost < best_cost:
            best_cost, best = cost, (offset, theta0)
    offset, theta0 = best
    new_angles, adjustments = [], []
    for i in range(N):
        k = (i - offset) % N
        target = (theta0 + k*S) % 360.0
        orig = meas[i]
        rel = w[i]
        alpha = min(max(rel * 0.8, 0.35), 0.85)
        delta = ((target - orig + 180.0) % 360.0) - 180.0
        adj = alpha * delta
        new_a = (orig + adj) % 360.0
        new_angles.append(new_a)
        adjustments.append(adj)
    rows["azimuth_deg_5_soft"] = [snap_deg(a, step=bin_size) for a in new_angles]
    rows["azimuth_adjustment_deg"] = [round(float(adj), 2) for adj in adjustments]
    rows["template_spacing_deg"] = S
    rows["spacing_used"] = f"{int(round(S))}°"
    key_cols = [c for c in ["network","earfcn_or_narfcn","site_key_inferred","pci_or_psi"] if c in df_site.columns]
    rows["__rowid__"] = range(len(rows))
    df_site2 = df_site.merge(rows[key_cols + ["__rowid__","azimuth_deg_5_soft","azimuth_adjustment_deg","template_spacing_deg","spacing_used"]], on=key_cols, how="left")
    df_site2.drop(columns=["__rowid__"], inplace=True)
    return df_site2

# --------------------- NO-ML pipeline ------------------------------
def weighted_centroid_top_rsrp(g: pd.DataFrame):
    if "rsrp_dbm" in g.columns and not g["rsrp_dbm"].isna().all():
        w = g["rsrp_dbm"].apply(lambda v: max(10**(v/10.0), 1e-13) if pd.notna(v) else 0.0)
    else:
        w = pd.Series(np.ones(len(g)), index=g.index)
    q = w.quantile(0.9)
    sel = g[w >= q] if q>0 else g
    if len(sel) < 10:
        if "rsrp_dbm" in g.columns:
            sel = g.nlargest(min(20, len(g)), "rsrp_dbm")
            w = sel["rsrp_dbm"].apply(lambda v: max(10**(v/10.0), 1e-13) if pd.notna(v) else 0.0)
        else:
            w = pd.Series(np.ones(len(sel)), index=sel.index)
    # SUBSET weights to selected rows to avoid dilution
    w_sel = w.loc[sel.index]
    W = float(w_sel.sum()) if float(w_sel.sum())>0 else 1.0
    lat_c = float((sel["lat"]*w_sel).sum()/W)
    lon_c = float((sel["lon"]*w_sel).sum()/W)
    med_dist = float(np.median([haversine(lat_c, lon_c, r.lat, r.lon) for r in sel.itertuples(index=False)]))
    return lat_c, lon_c, med_dist

def run_noml(input_path: str, outdir: str, sheet: str=None, min_samples:int=30, bin_size:int=5, soft_spacing:bool=True, use_ta:bool=False, make_map:bool=False, merge_sites:bool=False) -> Dict[str,str]:
    os.makedirs(outdir, exist_ok=True)
    base = os.path.splitext(os.path.basename(input_path))[0]
    ts = datetime.now().strftime("%Y%m%d_%H%M%S")
    df_raw = load_any(input_path, sheet)
    df = standardize_df(df_raw)
    audit_cols = [c for c in ["timestamp_utc","lat","lon","technology","network","band","band_mhz","earfcn_or_narfcn","pci_or_psi","rsrp_dbm","rsrq_db","sinr_db","ta"] if c in df.columns]
    audit = df[audit_cols].head(20000).copy()
    audit_path = os.path.join(outdir, f"{base}_{ts}_audit_preview.csv")
    audit.to_csv(audit_path, index=False)
    logging.info(f"Audit -> {audit_path}")
    group_cols = []
    if "network" in df.columns: group_cols.append("network")
    for gc in ["earfcn_or_narfcn","pci_or_psi"]:
        if gc in df.columns: group_cols.append(gc)
    if len(group_cols) < 2:
        raise ValueError("Need at least earfcn_or_narfcn and pci_or_psi (network optional).")
    pred_rows = []
    cellid_col = None
    for c in ["cell_id_global","cellid","cell_id","eci","ecgi","nrcgi","nr_cgi"]:
        if c in df.columns: cellid_col = c; break
    for keys, g in df.groupby(group_cols):
        g2 = g.dropna(subset=["lat","lon"])
        if len(g2) < min_samples: continue
        lat_c, lon_c, med_dist = weighted_centroid_top_rsrp(g2)
        if cellid_col and cellid_col in g2.columns:
            try:
                cell_id_rep = g2[cellid_col].dropna().astype(str).value_counts().idxmax()
            except:
                cell_id_rep = np.nan
        else:
            cell_id_rep = np.nan
        kd = {}
        if isinstance(keys, tuple):
            for i,k in enumerate(group_cols): kd[k]=keys[i]
        else:
            kd[group_cols[0]] = keys
        pred_rows.append({**kd, "samples": int(len(g2)), "lat_pred_firstcut": lat_c, "lon_pred_firstcut": lon_c, "median_sample_distance_m": med_dist, "cell_id_representative": cell_id_rep})
    pred_first = pd.DataFrame(pred_rows)
    if len(pred_first)==0:
        raise RuntimeError("No groups passed min_samples.")
    pred_first["site_key_inferred"] = pred_first["cell_id_representative"].apply(infer_site_key)
    site_group_cols = []
    if "network" in pred_first.columns: site_group_cols.append("network")
    for gc in ["earfcn_or_narfcn","site_key_inferred"]:
        if gc in pred_first.columns: site_group_cols.append(gc)
    if len(site_group_cols)>=2:
        pred_first["_w"] = pred_first["samples"].clip(lower=1)
        site_centroids = (pred_first.groupby(site_group_cols)
                          .apply(lambda g: pd.Series({"lat_site": float(np.average(g["lat_pred_firstcut"], weights=g["_w"])),
                                                      "lon_site": float(np.average(g["lon_pred_firstcut"], weights=g["_w"])),
                                                      "sector_count": len(g)})).reset_index())
        pred_first = pred_first.merge(site_centroids, on=site_group_cols, how="left")
    else:
        pred_first["lat_site"] = pred_first["lat_pred_firstcut"]
        pred_first["lon_site"] = pred_first["lon_pred_firstcut"]
        pred_first["sector_count"] = 1
    # azimuth per sector
    az_rows = []
    for r in pred_first.itertuples(index=False):
        m = pd.Series(True, index=df.index)
        if "network" in df.columns and hasattr(r, "network") and pd.notna(r.network): m &= (df["network"] == r.network)
        if "earfcn_or_narfcn" in df.columns and hasattr(r, "earfcn_or_narfcn"): m &= (df["earfcn_or_narfcn"] == r.earfcn_or_narfcn)
        if "pci_or_psi" in df.columns and hasattr(r, "pci_or_psi"): m &= (df["pci_or_psi"] == r.pci_or_psi)
        g = df[m].dropna(subset=["lat","lon"])
        if len(g) < 15:
            az5, beam, rel = (np.nan, np.nan, np.nan)
        else:
            az5, beam, rel = azimuth_histogram(g, r.lat_site, r.lon_site, bin_size=bin_size)
        az_rows.append({"azimuth_deg_5": az5, "beamwidth_deg_est": beam, "azimuth_reliability": rel})
    az_df = pd.DataFrame(az_rows)
    pred_out = pd.concat([pred_first.reset_index(drop=True), az_df], axis=1)
    
    pred_out.rename(columns={"lat_site":"lat_pred","lon_site":"lon_pred"}, inplace=True)
    # --- Global & site-level sanity guards ---
    try:
        import numpy as _np, math as _math, pandas as _pd

        def _hav(a,b,c,d):
            R=6371000.0
            phi1, phi2 = _math.radians(a), _math.radians(c)
            dphi = _math.radians(c-a); dl = _math.radians(d-b)
            x = _math.sin(dphi/2)**2 + _math.cos(phi1)*_math.cos(phi2)*_math.sin(dl/2)**2
            return 2*R*_math.asin(_math.sqrt(x))

        # Global input median and p95 radius
        lat_med_all = float(df["lat"].median())
        lon_med_all = float(df["lon"].median())
        dists_all = _np.array([_hav(lat_med_all, lon_med_all, r.lat, r.lon) for r in df.dropna(subset=["lat","lon"]).itertuples(index=False)])
        if dists_all.size > 0:
            rad95 = float(_np.percentile(dists_all, 95))
        else:
            rad95 = 5000.0

        # Site-group spread check: if sectors within a site spread too wide, abandon site-averaging
        key_cols_site = []
        if "network" in pred_out.columns: key_cols_site.append("network")
        for gc in ["earfcn_or_narfcn","site_key_inferred"]:
            if gc in pred_out.columns: key_cols_site.append(gc)
        bad_sites = 0
        if len(key_cols_site) >= 2:
            for sk, gg in pred_out.groupby(key_cols_site):
                # distances between each sector's firstcut and the averaged site location
                lat_s = float(gg["lat_pred"].iloc[0]); lon_s = float(gg["lon_pred"].iloc[0])
                d_spreads = [_hav(lat_s, lon_s, rr.lat_pred_firstcut, rr.lon_pred_firstcut) for rr in gg.itertuples(index=False)]
                if len(d_spreads) > 0 and (max(d_spreads) > 1500.0 or _np.median(d_spreads) > 800.0):
                    # revert this entire site group back to per-sector firstcut
                    idx = _pd.Series(True, index=pred_out.index)
                    for i,k in enumerate(key_cols_site):
                        idx &= pred_out[k].eq(gg[k].iloc[0])
                    pred_out.loc[idx, ["lat_pred","lon_pred"]] = pred_out.loc[idx, ["lat_pred_firstcut","lon_pred_firstcut"]].values
                    pred_out.loc[idx, "sector_count"] = 1
                    bad_sites += int(idx.sum())
        if bad_sites:
            logging.warning(f"Site-spread guard reverted {bad_sites} rows to per-sector firstcut (site spread too large).")

        # Per-sector local & global geofence checks
        corrected = 0
        for r in pred_out.itertuples(index=False):
            # group filter for local median
            m = _pd.Series(True, index=df.index)
            if "network" in df.columns and hasattr(r, "network") and _pd.notna(r.network): m &= (df["network"] == r.network)
            if "earfcn_or_narfcn" in df.columns and hasattr(r, "earfcn_or_narfcn"): m &= (df["earfcn_or_narfcn"] == r.earfcn_or_narfcn)
            if "pci_or_psi" in df.columns and hasattr(r, "pci_or_psi"): m &= (df["pci_or_psi"] == r.pci_or_psi)
            g = df[m].dropna(subset=["lat","lon"])
            # default: use existing pred
            lat_p, lon_p = float(r.lat_pred), float(r.lon_pred)
            bad = False
            # local check
            if len(g) >= 5:
                glat_med, glon_med = float(g["lat"].median()), float(g["lon"].median())
                dist_local = _hav(glat_med, glon_med, lat_p, lon_p)
                if dist_local > 1000.0:
                    bad = True
            # global check
            dist_global = _hav(lat_med_all, lon_med_all, lat_p, lon_p)
            if dist_global > (rad95 + 1000.0):
                bad = True
            if bad:
                idx = (_pd.Series(True, index=pred_out.index))
                if "network" in pred_out.columns and hasattr(r, "network"):
                    idx &= pred_out["network"].eq(getattr(r,"network",_np.nan))
                idx &= pred_out["earfcn_or_narfcn"].eq(getattr(r,"earfcn_or_narfcn",_np.nan))
                idx &= pred_out["pci_or_psi"].eq(getattr(r,"pci_or_psi",_np.nan))
                pred_out.loc[idx, ["lat_pred","lon_pred"]] = pred_out.loc[idx, ["lat_pred_firstcut","lon_pred_firstcut"]].values
                corrected += int(idx.sum())
        if corrected:
            logging.warning(f"Spatial geofence corrected {corrected} sector rows (outside local/global envelope).")
    except Exception as e:
        logging.warning(f"Spatial sanity/geofence checks skipped: {e}")
    
    # --- Spatial sanity guard per sector group ---
    # If final site centroid is >1 km from the group's raw-sample median, fall back to group's firstcut centroid.
    try:
        def haversine(lat1, lon1, lat2, lon2):
            import math
            R=6371000.0
            phi1, phi2 = math.radians(lat1), math.radians(lat2)
            dphi = math.radians(lat2-lat1)
            dl = math.radians(lon2-lon1)
            a = math.sin(dphi/2)**2 + math.cos(phi1)*math.cos(phi2)*math.sin(dl/2)**2
            return 2*R*math.asin(math.sqrt(a))

        corrected = 0
        for r in pred_out.itertuples(index=False):
            m = pd.Series(True, index=df.index)
            if "network" in df.columns and hasattr(r, "network") and pd.notna(r.network): m &= (df["network"] == r.network)
            if "earfcn_or_narfcn" in df.columns and hasattr(r, "earfcn_or_narfcn"): m &= (df["earfcn_or_narfcn"] == r.earfcn_or_narfcn)
            if "pci_or_psi" in df.columns and hasattr(r, "pci_or_psi"): m &= (df["pci_or_psi"] == r.pci_or_psi)
            g = df[m].dropna(subset=["lat","lon"])
            if len(g) < 5: 
                continue
            glat_med, glon_med = float(g["lat"].median()), float(g["lon"].median())
            dist = haversine(glat_med, glon_med, float(r.lat_pred), float(r.lon_pred))
            if dist > 1000.0:  # > 1 km away from the group's sample cloud -> suspicious
                idx = (pred_out["network"].eq(getattr(r,"network",np.nan)) if "network" in pred_out.columns else True) &                       (pred_out["earfcn_or_narfcn"].eq(getattr(r,"earfcn_or_narfcn",np.nan))) &                       (pred_out["pci_or_psi"].eq(getattr(r,"pci_or_psi",np.nan)))
                # fallback: use the group's firstcut centroid for this sector
                pred_out.loc[idx, ["lat_pred","lon_pred"]] = pred_out.loc[idx, ["lat_pred_firstcut","lon_pred_firstcut"]].values
                corrected += int(idx.sum())
        if corrected:
            logging.warning(f"Spatial sanity guard corrected {corrected} sector rows (>1km from sample cloud).")
    except Exception as e:
        logging.warning(f"Spatial sanity guard skipped: {e}")

    cols = [c for c in ["network","earfcn_or_narfcn","pci_or_psi","samples","lat_pred","lon_pred","azimuth_deg_5","beamwidth_deg_est","median_sample_distance_m","cell_id_representative","site_key_inferred","sector_count","azimuth_reliability"] if c in pred_out.columns]
    no_ta_path = os.path.join(outdir, f"{base}_{ts}_pred_main_no_ta.csv")
    pred_out[cols].to_csv(no_ta_path, index=False)
    logging.info(f"NO-ML -> {no_ta_path}")

    # soft spacing
    soft_path = None
    if soft_spacing:
        key_cols = []
        if "network" in pred_out.columns: key_cols.append("network")
        for gc in ["earfcn_or_narfcn","site_key_inferred"]:
            if gc in pred_out.columns: key_cols.append(gc)
        parts = []
        for _, g in pred_out.groupby(key_cols):
            parts.append(soft_equal_spacing(g, bin_size=bin_size))
        pred_soft = pd.concat(parts, ignore_index=True)
        pred_soft["azimuth_deg_label_soft"] = pred_soft["azimuth_deg_5_soft"].apply(lambda v: f"{int(v)} degree" if not pd.isna(v) else "")
        keep = [c for c in ["network","earfcn_or_narfcn","site_key_inferred","pci_or_psi","samples","lat_pred","lon_pred","azimuth_deg_5","azimuth_deg_5_soft","azimuth_deg_label_soft","azimuth_adjustment_deg","template_spacing_deg","beamwidth_deg_est","median_sample_distance_m","cell_id_representative","sector_count","azimuth_reliability","spacing_used"] if c in pred_soft.columns]
        soft_path = os.path.join(outdir, f"{base}_{ts}_pred_main_no_ta_soft.csv")
        pred_soft[keep].to_csv(soft_path, index=False)
        logging.info(f"NO-ML + soft -> {soft_path}")
    # optional TA refine (grid search)
    ta_path = None
    if use_ta and "ta" in df.columns and not df["ta"].dropna().empty:
        rows_ta = []
        for r in pred_out.itertuples(index=False):
            m = pd.Series(True, index=df.index)
            if "network" in df.columns and hasattr(r, "network") and pd.notna(r.network): m &= (df["network"] == r.network)
            if "earfcn_or_narfcn" in df.columns and hasattr(r, "earfcn_or_narfcn"): m &= (df["earfcn_or_narfcn"] == r.earfcn_or_narfcn)
            if "pci_or_psi" in df.columns and hasattr(r, "pci_or_psi"): m &= (df["pci_or_psi"] == r.pci_or_psi)
            g = df[m].dropna(subset=["lat","lon","ta"])
            if len(g) < 25:
                rows_ta.append({**r._asdict(), "ta_refine_abs_error_m": np.nan})
                continue
            base_lat, base_lon = float(r.lat_pred), float(r.lon_pred)
            g = g.assign(ta_m = g["ta"] * 78.0)
            best = (1e18, base_lat, base_lon)
            for dN in range(-200, 201, 50):
                for dE in range(-200, 201, 50):
                    lat_try, lon_try = meters_to_offsets(dN, dE, base_lat)
                    dists = np.array([haversine(lat_try, lon_try, rr.lat, rr.lon) for rr in g.itertuples(index=False)])
                    loss = float(np.mean(np.abs(dists - g["ta_m"].values)))
                    if loss < best[0]: best = (loss, lat_try, lon_try)
            rdict = r._asdict(); rdict["lat_pred"] = best[1]; rdict["lon_pred"] = best[2]; rdict["ta_refine_abs_error_m"] = best[0]
            rows_ta.append(rdict)
        pred_ta = pd.DataFrame(rows_ta)
        ta_path = os.path.join(outdir, f"{base}_{ts}_pred_main_ta_refined.csv")
        pred_ta.to_csv(ta_path, index=False)
        logging.info(f"NO-ML TA refine -> {ta_path}")
    # map
    map_path = None
    if make_map:
        try:
            import folium
            use_df = pred_soft if (soft_spacing and soft_path) else pred_out
            m = folium.Map(location=[float(use_df["lat_pred"].median()), float(use_df["lon_pred"].median())], zoom_start=13)
            for r in use_df.itertuples(index=False):
                lat, lon = float(r.lat_pred), float(r.lon_pred)
                az = getattr(r, "azimuth_deg_5_soft", np.nan) if (soft_spacing and soft_path) else getattr(r, "azimuth_deg_5", np.nan)
                tooltip = f"{getattr(r,'network','')} | {getattr(r,'earfcn_or_narfcn','')} | PCI {getattr(r,'pci_or_psi','')} | Az {az}°"
                folium.CircleMarker([lat,lon], radius=4, fill=True, tooltip=tooltip).add_to(m)
            map_path = os.path.join(outdir, f"{base}_{ts}_map.html")
            m.save(map_path)
            logging.info(f"Map -> {map_path}")
        except Exception as e:
            logging.warning(f"Map generation skipped: {e}")
    # === MODIFIED ===
    # Return the final dataframe along with the paths
    final_df = pred_soft if (soft_spacing and soft_path) else pred_out
    return {"audit": audit_path, "no_ta": no_ta_path, "soft": soft_path, "ta": ta_path, "map": map_path, "dataframe": final_df}

# --------------------- ML pipeline (continual training + imputer) --
FEATURE_CANDIDATES = ["rsrp_dbm","rsrq_db","sinr_db","rssi","band_mhz","earfcn_or_narfcn","speed_kmh","heading_deg"]
CATEGORICALS = ["technology","network"]

def build_features(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()
    for c in FEATURE_CANDIDATES:
        if c in df.columns: df[c] = df[c].apply(to_num)
    if "rsrp_dbm" in df.columns:
        df["rsrp_lin"] = df["rsrp_dbm"].apply(lambda v: 10**(v/10.0) if pd.notna(v) else np.nan)
    if "sinr_db" in df.columns and "rsrp_dbm" in df.columns:
        df["rsrp_sinr"] = df["sinr_db"].fillna(0) + df["rsrp_dbm"].fillna(-120)
    if "rsrq_db" in df.columns and "rsrp_dbm" in df.columns:
        df["rsrp_rsrq"] = df["rsrq_db"].fillna(0) + df["rsrp_dbm"].fillna(-120)
    for cat in CATEGORICALS:
        if cat in df.columns: df[cat] = df[cat].astype(str).str.lower()
    return df

def select_feature_matrix(df: pd.DataFrame):
    cols = [c for c in FEATURE_CANDIDATES + ["rsrp_lin","rsrp_sinr","rsrp_rsrq"] if c in df.columns]
    used_cats = [c for c in CATEGORICALS if c in df.columns]
    X = df[cols + used_cats].copy()
    X = pd.get_dummies(X, columns=used_cats, dummy_na=True)
    X = X.replace([np.inf, -np.inf], np.nan)
    return X, list(X.columns)

# Replay persistence
def bundle_dir_from_model(model_path:str) -> str:
    return os.path.dirname(os.path.abspath(model_path))
def replay_path_from_dir(model_dir:str) -> str:
    return os.path.join(model_dir, "distance_model_replay.csv.gz")
def save_bundle(model, imputer, features:List[str], outdir:str, version:int, extra_meta:Dict) -> str:
    bundle = {"model": model, "imputer": imputer, "features": features, "version": version, "meta": extra_meta}
    model_path = os.path.join(outdir, "distance_model.joblib")
    joblib.dump(bundle, model_path)
    return model_path
def load_bundle(model_path:str):
    return joblib.load(model_path)
def hash_rows(df: pd.DataFrame, cols: List[str]) -> pd.Series:
    def hrow(row):
        s = "|".join([str(row.get(c, "")) for c in cols])
        return hashlib.md5(s.encode("utf-8")).hexdigest()
    return df.apply(hrow, axis=1)
def append_to_replay(replay_path:str, X_new: pd.DataFrame, y_new: pd.Series):
    if os.path.exists(replay_path):
        rep = pd.read_csv(replay_path, compression="gzip")
        y_col = "target_distance_m"
        cols_union = list(sorted(set(rep.columns) | set(X_new.columns) | {y_col}))
        for c in cols_union:
            if c not in rep.columns: rep[c] = 0
            if c not in X_new.columns and c != y_col: X_new[c] = 0
        rep = rep[cols_union]
        X_new = X_new[cols_union[:-1]] if cols_union[-1] == y_col else X_new[cols_union]
        new = X_new.copy(); new[y_col] = y_new.values
        merged = pd.concat([rep, new], ignore_index=True)
        feat_cols = [c for c in merged.columns if c != y_col]
        merged["_h"] = hash_rows(merged, feat_cols)
        merged = merged.drop_duplicates(subset="_h").drop(columns="_h")
        merged.to_csv(replay_path, index=False, compression="gzip")
        X_all = merged[feat_cols].copy(); y_all = merged[y_col].copy()
    else:
        y_col = "target_distance_m"
        new = X_new.copy(); new[y_col] = y_new.values
        new.to_csv(replay_path, index=False, compression="gzip")
        X_all = X_new.copy(); y_all = y_new.copy()
    return X_all, y_all

def train_or_update_model(train_df: pd.DataFrame, outdir:str, existing_bundle_path:str=None):
    if not SKLEARN_AVAILABLE:
        raise RuntimeError("scikit-learn/joblib not available. Install: pip install scikit-learn joblib")
    # Validate labels
    label_lat_col = None; label_lon_col = None
    for cand in ["sector_lat","site_lat","lat_site","site_latitude"]:
        if cand in train_df.columns: label_lat_col = cand; break
    for cand in ["sector_lon","site_lon","lon_site","site_longitude"]:
        if cand in train_df.columns: label_lon_col = cand; break
    if not label_lat_col or not label_lon_col:
        raise ValueError("Training needs sector/site coordinates (e.g., sector_lat, sector_lon).")
    tr = train_df.dropna(subset=["lat","lon", label_lat_col, label_lon_col]).copy()
    tr["target_distance_m"] = [haversine(r.lat, r.lon, getattr(r, label_lat_col), getattr(r, label_lon_col)) for r in tr.itertuples(index=False)]
    # Features
    tr = build_features(tr)
    X_new, feat_new = select_feature_matrix(tr)
    y_new = tr["target_distance_m"]
    # Replay path (prefer bundle dir if provided)
    replay_path = replay_path_from_dir(outdir if existing_bundle_path is None else bundle_dir_from_model(existing_bundle_path))
    X_all, y_all = append_to_replay(replay_path, X_new, y_new)
    features = list(X_all.columns)
    # Imputer (fit on all replay data)
    imputer = SimpleImputer(strategy="median")
    X_all_imp = pd.DataFrame(imputer.fit_transform(X_all), columns=features)
    # CV
    kf = KFold(n_splits=min(5, max(2, int(len(X_all_imp) / 500))), shuffle=True, random_state=42)
    maes = []
    for tr_idx, va_idx in kf.split(X_all_imp):
        model = RandomForestRegressor(n_estimators=300, max_depth=None, min_samples_leaf=2, random_state=42, n_jobs=-1)
        model.fit(X_all_imp.iloc[tr_idx], y_all.iloc[tr_idx])
        pred = model.predict(X_all_imp.iloc[va_idx])
        maes.append(mean_absolute_error(y_all.iloc[va_idx], pred))
    cv_mae = float(np.mean(maes)) if len(maes)>0 else np.nan
    cv_rmse = float(np.sqrt(np.mean((model.predict(X_all_imp) - y_all.values)**2))) if len(X_all_imp)>0 else np.nan
    # Final fit
    model = RandomForestRegressor(n_estimators=600, max_depth=None, min_samples_leaf=2, random_state=42, n_jobs=-1)
    model.fit(X_all_imp, y_all)
    # Versioning
    version = 1
    if existing_bundle_path and os.path.exists(existing_bundle_path):
        try:
            old = load_bundle(existing_bundle_path)
            version = int(old.get("version", 1)) + 1
        except Exception:
            version = 1
    meta = {"cv_mae_m": cv_mae, "cv_rmse_m": cv_rmse, "n_train": int(len(X_all_imp)), "replay_path": replay_path, "timestamp": datetime.now().isoformat()}
    model_path = save_bundle(model, imputer, features, outdir, version, meta)
    return model, imputer, {"model_path": model_path, **meta}

def solve_site_from_predicted_ranges(samples: pd.DataFrame, lat0: float, lon0: float,
                                     start_step_m: float = 300.0, min_step_m: float = 10.0):
    def loss(latc, lonc):
        d = np.array([haversine(latc, lonc, r.lat, r.lon) for r in samples.itertuples(index=False)])
        rhat = samples["pred_range_m"].values
        if "rsrp_dbm" in samples.columns and not samples["rsrp_dbm"].isna().all():
            w = np.array([max(10**(v/10.0), 1e-13) if not pd.isna(v) else 0.0 for v in samples["rsrp_dbm"].values])
        else:
            w = np.ones(len(samples))
        return float(np.average(np.abs(d - rhat), weights=w))
    best_lat, best_lon = lat0, lon0
    best_loss = loss(best_lat, best_lon)
    step = start_step_m
    dirs = [(1,0),(-1,0),(0,1),(0,-1),(1,1),(1,-1),(-1,1),(-1,-1)]
    while step >= min_step_m:
        improved = False
        for dn, de in dirs:
            lat_try, lon_try = meters_to_offsets(dn*step, de*step, best_lat)
            cur_loss = loss(lat_try, lon_try)
            if cur_loss + 1e-6 < best_loss:
                best_lat, best_lon, best_loss = lat_try, lon_try, cur_loss
                improved = True
        if not improved:
            step /= 2.0
    return best_lat, best_lon, best_loss

def run_ml(train_path: str=None, model_path: str=None, update_model: bool=False, input_path: str=None, outdir: str=None,
           sheet_train:str=None, sheet_input:str=None, min_samples:int=30, bin_size:int=5, soft_spacing:bool=True, make_map:bool=False,
           eval_path: str=None, sheet_eval: str=None, no_ml_merge: bool=False):
    if not SKLEARN_AVAILABLE:
        raise RuntimeError("scikit-learn/joblib not available. Install: pip install scikit-learn joblib")
    if input_path is None or outdir is None:
        raise ValueError("input_path and outdir are required")
    os.makedirs(outdir, exist_ok=True)
    base_in = os.path.splitext(os.path.basename(input_path))[0]
    ts = datetime.now().strftime("%Y%m%d_%H%M%S")
    # Load input
    in_raw = load_any(input_path, sheet_input)
    df = standardize_df(in_raw)
    in_audit = df.head(20000)
    in_audit_path = os.path.join(outdir, f"{base_in}_{ts}_audit_infer.csv")
    in_audit.to_csv(in_audit_path, index=False)
    logging.info(f"Audit infer -> {in_audit_path}")
    # Model: load or train/update
    if model_path and not update_model:
        logging.info(f"Loading model: {model_path}")
        bundle = load_bundle(model_path)
        model = bundle["model"]; tr_feats = bundle["features"]; imputer = bundle.get("imputer", None)
        bundle_meta = bundle.get("meta", {})
    else:
        if train_path is None:
            raise ValueError("Provide --train to fit a model or --model to load one (with --update-model to update).")
        tr_raw = load_any(train_path, sheet_train)
        tr = standardize_df(tr_raw)
        model, imputer, meta = train_or_update_model(tr, outdir, existing_bundle_path=model_path)
        logging.info(f"Trained/updated model -> {meta['model_path']} | CV MAE ≈ {meta.get('cv_mae_m', np.nan):.2f} m | CV RMSE ≈ {meta.get('cv_rmse_m', np.nan):.2f} m | n_train={meta.get('n_train')}")
        bundle_meta = meta
        tr_feats = load_bundle(meta["model_path"])["features"]
    # Predict ranges
    feats_in = build_features(df.copy())
    X_in, featnames = select_feature_matrix(feats_in)
    for col in tr_feats:
        if col not in X_in.columns: X_in[col] = 0
    X_in = X_in[tr_feats]
    X_in = X_in.replace([np.inf, -np.inf], np.nan)
    if imputer is None:
        logging.warning("Bundle has no imputer; fitting a temporary median imputer on inference features.")
        imputer = SimpleImputer(strategy="median").fit(X_in)
    X_in_imp = pd.DataFrame(imputer.transform(X_in), columns=tr_feats)
    df["pred_range_m"] = model.predict(X_in_imp)
    # group keys
    group_cols = []
    if "network" in df.columns: group_cols.append("network")
    for gc in ["earfcn_or_narfcn","pci_or_psi"]:
        if gc in df.columns: group_cols.append(gc)
    if len(group_cols) < 2:
        raise ValueError("Need at least earfcn_or_narfcn and pci_or_psi (network optional).")
    # initial point
    def weighted_centroid(g):
        if "rsrp_dbm" in g.columns and not g["rsrp_dbm"].isna().all():
            w = g["rsrp_dbm"].apply(lambda v: max(10**(v/10.0), 1e-13) if pd.notna(v) else 0.0)
        else:
            w = pd.Series(np.ones(len(g)), index=g.index)
        q = w.quantile(0.9)
        sel = g[w >= q] if q>0 else g
        if len(sel) < 10:
            if "rsrp_dbm" in g.columns:
                sel = g.nlargest(min(20, len(g)), "rsrp_dbm")
                w = sel["rsrp_dbm"].apply(lambda v: max(10**(v/10.0), 1e-13) if pd.notna(v) else 0.0)
            else:
                w = pd.Series(np.ones(len(sel)), index=sel.index)
        w_sel = w.loc[sel.index]
        W = float(w_sel.sum()) if float(w_sel.sum())>0 else 1.0
        lat0 = float((sel["lat"]*w_sel).sum()/W); lon0 = float((sel["lon"]*w_sel).sum()/W)
        return lat0, lon0
    # solve
    pred_rows = []
    for keys, g in df.groupby(group_cols):
        g2 = g.dropna(subset=["lat","lon","pred_range_m"])
        if len(g2) < max(20, min_samples): continue
        lat0, lon0 = weighted_centroid(g2)
        lat_hat, lon_hat, loss_mae = solve_site_from_predicted_ranges(g2, lat0, lon0)
        az5, beam_deg, rel = azimuth_histogram(g2, lat_hat, lon_hat, bin_size=bin_size)
        kd = {}
        if isinstance(keys, tuple):
            for i,k in enumerate(group_cols): kd[k] = keys[i]
        else:
            kd[group_cols[0]] = keys
        pred_rows.append({**kd, "samples": int(len(g2)), "lat_pred": lat_hat, "lon_pred": lon_hat, "azimuth_deg_5": az5, "beamwidth_deg_est": beam_deg, "azimuth_reliability": rel, "range_mae_m": float(loss_mae)})
    pred_df = pd.DataFrame(pred_rows)
    if len(pred_df)==0:
        raise RuntimeError("No sector groups produced predictions.")
    # Save per-sector predictions
    per_sector_path = os.path.join(outdir, f"{base_in}_{ts}_pred_ml_per_sector.csv")
    pred_df.to_csv(per_sector_path, index=False)
    logging.info(f"ML (per-sector) -> {per_sector_path}")
    # cell_id enrichment
    cellid_col = None
    for c in ["cell_id_representative","cell_id_global","cellid","cell_id","eci","ecgi","nrcgi","nr_cgi"]:
        if c in df.columns: cellid_col = c; break
    group_cols_for_map = group_cols.copy()
    if cellid_col:
        cellmap = (df.groupby(group_cols)[cellid_col].agg(lambda s: s.dropna().astype(str).value_counts().idxmax() if s.dropna().size>0 else np.nan).reset_index().rename(columns={cellid_col:"cell_id_representative"}))
        pred_df = pred_df.merge(cellmap, on=group_cols, how="left")
        pred_df["site_key_inferred"] = pred_df["cell_id_representative"].apply(infer_site_key)
    else:
        pred_df["cell_id_representative"] = np.nan
        pred_df["site_key_inferred"] = np.nan
    # --- Enforce same site lat/lon across sectors (ML) unless disabled ---
    if not no_ml_merge:
        site_keys = [c for c in ["network","earfcn_or_narfcn","site_key_inferred"] if c in pred_df.columns]
        if len(site_keys) >= 2:
            pred_df["sector_count"] = pred_df.groupby(site_keys)["pci_or_psi"].transform("nunique").fillna(1).astype(int)
            # robust center = median of sector centers
            site_centers = pred_df.groupby(site_keys).agg(lat_site=("lat_pred","median"), lon_site=("lon_pred","median")).reset_index()
            pred_df = pred_df.merge(site_centers, on=site_keys, how="left")
            pred_df["lat_pred"] = pred_df["lat_site"].fillna(pred_df["lat_pred"])
            pred_df["lon_pred"] = pred_df["lon_site"].fillna(pred_df["lon_pred"])
            # compute site spread for info
            def _hav(a,b,c,d):
                R=6371000.0
                phi1, phi2 = math.radians(a), math.radians(c)
                dphi = math.radians(c-a); dl = math.radians(d-b)
                x = math.sin(dphi/2)**2 + math.cos(phi1)*math.cos(phi2)*math.sin(dl/2)**2
                return 2*R*math.asin(math.sqrt(x))
            def _spread(group):
                latc, lonc = float(group["lat_pred"].iloc[0]), float(group["lon_pred"].iloc[0])
                return pd.Series({"site_spread_m": max([_hav(latc, lonc, float(r.lat_pred), float(r.lon_pred)) for r in group.itertuples(index=False)]) if len(group)>0 else 0.0})
            spread = pred_df.groupby(site_keys).apply(_spread).reset_index()
            pred_df = pred_df.merge(spread, on=site_keys, how="left")
        else:
            pred_df["site_spread_m"] = np.nan
            pred_df["sector_count"] = 1
    else:
        pred_df["site_spread_m"] = np.nan
        pred_df["sector_count"] = 1
    # soft spacing
    soft_path = None
    key_cols = [c for c in ["network","earfcn_or_narfcn","site_key_inferred"] if c in pred_df.columns]
    if soft_spacing and len(key_cols)>0:
        parts = []
        for _, g in pred_df.groupby(key_cols):
            parts.append(soft_equal_spacing(g, bin_size=bin_size))
        pred_soft = pd.concat(parts, ignore_index=True)
        pred_soft["azimuth_deg_label_soft"] = pred_soft["azimuth_deg_5_soft"].apply(lambda v: f"{int(v)} degree" if not pd.isna(v) else "")
    else:
        pred_soft = pred_df.copy()
    # save
    no_ta_path = os.path.join(outdir, f"{base_in}_{ts}_pred_ml_no_ta.csv")
    soft_path = os.path.join(outdir, f"{base_in}_{ts}_pred_ml_no_ta_soft.csv")
    pred_df.to_csv(no_ta_path, index=False)
    pred_soft.to_csv(soft_path, index=False)
    # Persist training CV metrics if available
    try:
        if bundle_meta:
            metrics_path = os.path.join(outdir, f"{base_in}_{ts}_ml_metrics.json")
            with open(metrics_path, "w") as f:
                json.dump({"cv": bundle_meta}, f, indent=2)
            logging.info(f"Saved ML CV metrics -> {metrics_path}")
    except Exception:
        pass
    logging.info(f"ML -> {no_ta_path} and {soft_path}")
    # map (optional)
    map_path = None
    if make_map:
        try:
            import folium
            use_df = pred_soft if soft_spacing else pred_df
            m = folium.Map(location=[float(use_df["lat_pred"].median()), float(use_df["lon_pred"].median())], zoom_start=13)
            for r in use_df.itertuples(index=False):
                lat, lon = float(r.lat_pred), float(r.lon_pred)
                az = getattr(r, "azimuth_deg_5_soft", np.nan) if soft_spacing else getattr(r, "azimuth_deg_5", np.nan)
                tooltip = f"{getattr(r,'network','')} | {getattr(r,'earfcn_or_narfcn','')} | PCI {getattr(r,'pci_or_psi','')} | Az {az}°"
                folium.CircleMarker([lat,lon], radius=4, fill=True, tooltip=tooltip).add_to(m)
            map_path = os.path.join(outdir, f"{base_in}_{ts}_map.html")
            m.save(map_path)
            logging.info(f"Map -> {map_path}")
        except Exception as e:
            logging.warning(f"Map generation skipped: {e}")
    # === MODIFIED ===
    # Return the final dataframe along with the paths
    final_df = pred_soft if soft_spacing else pred_df
    return {"no_ta": no_ta_path, "soft": soft_path, "map": map_path, "per_sector": per_sector_path, "dataframe": final_df}

# ----------------------------- CLI ---------------------------------
def main():
    ap = argparse.ArgumentParser(description="Unified Site & Sector Locator Tool (NO-ML & ML w/ Continual Training, NaN-safe, site-merge)")
    ap.add_argument("--method", choices=["noml","ml"], required=True, help="Which method to run")
    ap.add_argument("-i","--input", help="Input CSV/XLSX with drive-test samples (required for both methods)")
    ap.add_argument("-o","--outdir", required=True, help="Output directory")
    ap.add_argument("--sheet", default=None, help="Excel sheet for NO-ML input file")
    ap.add_argument("--sheet-train", default=None, help="Excel sheet for ML train file")
    ap.add_argument("--sheet-input", default=None, help="Excel sheet for ML input file")
    ap.add_argument("--min-samples", type=int, default=30, help="Min samples per (network,EARFCN,PCI) group")
    ap.add_argument("--bin-size", type=int, default=5, choices=[1,3,5,10,15], help="Azimuth histogram bin size (deg)")
    ap.add_argument("--soft-spacing", action="store_true", help="Soft enforce ~360/N spacing per site")
    ap.add_argument("--use-ta", action="store_true", help="(NO-ML only) Grid-search TA refine")
    ap.add_argument("--make-map", action="store_true", help="Export Folium HTML map")
    # ML specific
    ap.add_argument("--train", help="(ML) Labeled truth CSV/XLSX with sector_lat/sector_lon")
    ap.add_argument("--model", help="(ML) Pre-trained joblib model bundle")
    ap.add_argument("--update-model", action="store_true", help="(ML) Update existing model with new --train data (continual training)")
    ap.add_argument("--eval", help="(ML) Optional labeled eval CSV/XLSX to compute site-level metrics")
    ap.add_argument("--sheet-eval", default=None, help="Excel sheet for ML eval file")
    ap.add_argument("--no-ml-merge", action="store_true", help="Disable ML site merge (debug only)")

    args = ap.parse_args()

    setup_logger(args.outdir, tag=f"{args.method}")
    logging.info(f"Args: {vars(args)}")
    try:
        if args.method == "noml":
            if not args.input: raise ValueError("--input is required for NO-ML")
            outs = run_noml(args.input, args.outdir, sheet=args.sheet, min_samples=args.min_samples, bin_size=args.bin_size, soft_spacing=args.soft_spacing, use_ta=args.use_ta, make_map=args.make_map, merge_sites=args.soft_spacing)
        else:
            if not args.input: raise ValueError("--input is required for ML")
            outs = run_ml(train_path=args.train, model_path=args.model, update_model=args.update_model, input_path=args.input, outdir=args.outdir, sheet_train=args.sheet_train, sheet_input=args.sheet_input, min_samples=args.min_samples, bin_size=args.bin_size, soft_spacing=args.soft_spacing, make_map=args.make_map, eval_path=args.eval, sheet_eval=args.sheet_eval, no_ml_merge=args.no_ml_merge)
        logging.info("Done.")
        for k,v in outs.items():
            if v: logging.info(f"{k}: {v}")
    except Exception as e:
        logging.exception(f"FAILED: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()
